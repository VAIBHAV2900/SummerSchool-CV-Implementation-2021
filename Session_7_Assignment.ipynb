{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session-7_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6061420ea0e54e2f9d6396fced06a8ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_674784d3b8d74d7f84ddeb1c1007b13f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_871625b1a9534b559d5cd63fb42c71d5",
              "IPY_MODEL_d093234b53714c6fbcd01917797ce552"
            ]
          }
        },
        "674784d3b8d74d7f84ddeb1c1007b13f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "871625b1a9534b559d5cd63fb42c71d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_290763596a6b4d5d9c3a63d1c55b79f6",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 46830571,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 46830571,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_20052519d0b54fd082d75b812a250be0"
          }
        },
        "d093234b53714c6fbcd01917797ce552": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_dbfde830bd5a4452b45b442e479cdf89",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 65.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4db104d66ce943db9de97d3d85d2c824"
          }
        },
        "290763596a6b4d5d9c3a63d1c55b79f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "20052519d0b54fd082d75b812a250be0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dbfde830bd5a4452b45b442e479cdf89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4db104d66ce943db9de97d3d85d2c824": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VAIBHAV2900/SummerSchool-CV-Implementation-2021/blob/main/Session_7_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diyGYvVzkzVq"
      },
      "source": [
        "#TASK 1\n",
        "\n",
        "1.   PyTorch provides two data primitives: `torch.utils.data.DataLoader` and `torch.utils.data.Dataset` that allow you to use pre-loaded datasets as well as your own data. Use the `torchvision.datasets` to import preloaded Dataset [`CIFAR`](https://pytorch.org/vision/stable/datasets.html#cifar) to train and then finetune the ResNet18 model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJgrC7WpnGIf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "6061420ea0e54e2f9d6396fced06a8ed",
            "674784d3b8d74d7f84ddeb1c1007b13f",
            "871625b1a9534b559d5cd63fb42c71d5",
            "d093234b53714c6fbcd01917797ce552",
            "290763596a6b4d5d9c3a63d1c55b79f6",
            "20052519d0b54fd082d75b812a250be0",
            "dbfde830bd5a4452b45b442e479cdf89",
            "4db104d66ce943db9de97d3d85d2c824"
          ]
        },
        "outputId": "412953bf-4ca0-4880-aa39-a1628f645097"
      },
      "source": [
        "#Loading in Resnet18 pre-trained Model\n",
        "\n",
        "''' Enter your code here''' \n",
        "#Loading in pre-trained Model\n",
        "from torchvision import models\n",
        "model = models.resnet18(pretrained = True)\n",
        "#You can use ```dir(models)``` to see various models available for Transfer Learning"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6061420ea0e54e2f9d6396fced06a8ed",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=46830571.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjzwC2bQn2gb"
      },
      "source": [
        "#Freeze the model weights\n",
        "'''Enter your code here'''\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jW3IqytOWRVg",
        "outputId": "a18c52c3-acae-4a72-b4ed-b7b5639c717c"
      },
      "source": [
        "model"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhrR_OBLRlHd",
        "outputId": "3d41a433-1335-40e2-89e1-82a9ca9e5d55"
      },
      "source": [
        "from torchsummary import summary\n",
        "summary(model, input_size=(3, 32, 32), batch_size=8, device='cpu')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [8, 64, 16, 16]           9,408\n",
            "       BatchNorm2d-2            [8, 64, 16, 16]             128\n",
            "              ReLU-3            [8, 64, 16, 16]               0\n",
            "         MaxPool2d-4              [8, 64, 8, 8]               0\n",
            "            Conv2d-5              [8, 64, 8, 8]          36,864\n",
            "       BatchNorm2d-6              [8, 64, 8, 8]             128\n",
            "              ReLU-7              [8, 64, 8, 8]               0\n",
            "            Conv2d-8              [8, 64, 8, 8]          36,864\n",
            "       BatchNorm2d-9              [8, 64, 8, 8]             128\n",
            "             ReLU-10              [8, 64, 8, 8]               0\n",
            "       BasicBlock-11              [8, 64, 8, 8]               0\n",
            "           Conv2d-12              [8, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-13              [8, 64, 8, 8]             128\n",
            "             ReLU-14              [8, 64, 8, 8]               0\n",
            "           Conv2d-15              [8, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-16              [8, 64, 8, 8]             128\n",
            "             ReLU-17              [8, 64, 8, 8]               0\n",
            "       BasicBlock-18              [8, 64, 8, 8]               0\n",
            "           Conv2d-19             [8, 128, 4, 4]          73,728\n",
            "      BatchNorm2d-20             [8, 128, 4, 4]             256\n",
            "             ReLU-21             [8, 128, 4, 4]               0\n",
            "           Conv2d-22             [8, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-23             [8, 128, 4, 4]             256\n",
            "           Conv2d-24             [8, 128, 4, 4]           8,192\n",
            "      BatchNorm2d-25             [8, 128, 4, 4]             256\n",
            "             ReLU-26             [8, 128, 4, 4]               0\n",
            "       BasicBlock-27             [8, 128, 4, 4]               0\n",
            "           Conv2d-28             [8, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-29             [8, 128, 4, 4]             256\n",
            "             ReLU-30             [8, 128, 4, 4]               0\n",
            "           Conv2d-31             [8, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-32             [8, 128, 4, 4]             256\n",
            "             ReLU-33             [8, 128, 4, 4]               0\n",
            "       BasicBlock-34             [8, 128, 4, 4]               0\n",
            "           Conv2d-35             [8, 256, 2, 2]         294,912\n",
            "      BatchNorm2d-36             [8, 256, 2, 2]             512\n",
            "             ReLU-37             [8, 256, 2, 2]               0\n",
            "           Conv2d-38             [8, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-39             [8, 256, 2, 2]             512\n",
            "           Conv2d-40             [8, 256, 2, 2]          32,768\n",
            "      BatchNorm2d-41             [8, 256, 2, 2]             512\n",
            "             ReLU-42             [8, 256, 2, 2]               0\n",
            "       BasicBlock-43             [8, 256, 2, 2]               0\n",
            "           Conv2d-44             [8, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-45             [8, 256, 2, 2]             512\n",
            "             ReLU-46             [8, 256, 2, 2]               0\n",
            "           Conv2d-47             [8, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-48             [8, 256, 2, 2]             512\n",
            "             ReLU-49             [8, 256, 2, 2]               0\n",
            "       BasicBlock-50             [8, 256, 2, 2]               0\n",
            "           Conv2d-51             [8, 512, 1, 1]       1,179,648\n",
            "      BatchNorm2d-52             [8, 512, 1, 1]           1,024\n",
            "             ReLU-53             [8, 512, 1, 1]               0\n",
            "           Conv2d-54             [8, 512, 1, 1]       2,359,296\n",
            "      BatchNorm2d-55             [8, 512, 1, 1]           1,024\n",
            "           Conv2d-56             [8, 512, 1, 1]         131,072\n",
            "      BatchNorm2d-57             [8, 512, 1, 1]           1,024\n",
            "             ReLU-58             [8, 512, 1, 1]               0\n",
            "       BasicBlock-59             [8, 512, 1, 1]               0\n",
            "           Conv2d-60             [8, 512, 1, 1]       2,359,296\n",
            "      BatchNorm2d-61             [8, 512, 1, 1]           1,024\n",
            "             ReLU-62             [8, 512, 1, 1]               0\n",
            "           Conv2d-63             [8, 512, 1, 1]       2,359,296\n",
            "      BatchNorm2d-64             [8, 512, 1, 1]           1,024\n",
            "             ReLU-65             [8, 512, 1, 1]               0\n",
            "       BasicBlock-66             [8, 512, 1, 1]               0\n",
            "AdaptiveAvgPool2d-67             [8, 512, 1, 1]               0\n",
            "           Linear-68                  [8, 1000]         513,000\n",
            "================================================================\n",
            "Total params: 11,689,512\n",
            "Trainable params: 0\n",
            "Non-trainable params: 11,689,512\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.09\n",
            "Forward/backward pass size (MB): 10.34\n",
            "Params size (MB): 44.59\n",
            "Estimated Total Size (MB): 55.03\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSoIrAU6n76g"
      },
      "source": [
        "#Add the fully connected layer\n",
        "#Here replace num_classes with number of classes you are classifying \n",
        "import torch.nn as nn\n",
        "\n",
        "# num_ftrs = model.AuxLogits.fc.in_features\n",
        "# model.AuxLogits.fc = nn.Linear(num_ftrs, 10)  #Auxillary Loss layer\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 10)          #Primary Loss layer\n",
        "model = model.to('cuda')  #Moving Model to GPU\n",
        "#Check model summary once to ensure the changes you made"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGTdpS-jRtEU",
        "outputId": "d6ba312e-2997-452b-a98d-d5c0c3be2ca2"
      },
      "source": [
        "from torchsummary import summary\n",
        "summary(model, input_size=(3, 32, 32), batch_size=8, device='cuda')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [8, 64, 16, 16]           9,408\n",
            "       BatchNorm2d-2            [8, 64, 16, 16]             128\n",
            "              ReLU-3            [8, 64, 16, 16]               0\n",
            "         MaxPool2d-4              [8, 64, 8, 8]               0\n",
            "            Conv2d-5              [8, 64, 8, 8]          36,864\n",
            "       BatchNorm2d-6              [8, 64, 8, 8]             128\n",
            "              ReLU-7              [8, 64, 8, 8]               0\n",
            "            Conv2d-8              [8, 64, 8, 8]          36,864\n",
            "       BatchNorm2d-9              [8, 64, 8, 8]             128\n",
            "             ReLU-10              [8, 64, 8, 8]               0\n",
            "       BasicBlock-11              [8, 64, 8, 8]               0\n",
            "           Conv2d-12              [8, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-13              [8, 64, 8, 8]             128\n",
            "             ReLU-14              [8, 64, 8, 8]               0\n",
            "           Conv2d-15              [8, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-16              [8, 64, 8, 8]             128\n",
            "             ReLU-17              [8, 64, 8, 8]               0\n",
            "       BasicBlock-18              [8, 64, 8, 8]               0\n",
            "           Conv2d-19             [8, 128, 4, 4]          73,728\n",
            "      BatchNorm2d-20             [8, 128, 4, 4]             256\n",
            "             ReLU-21             [8, 128, 4, 4]               0\n",
            "           Conv2d-22             [8, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-23             [8, 128, 4, 4]             256\n",
            "           Conv2d-24             [8, 128, 4, 4]           8,192\n",
            "      BatchNorm2d-25             [8, 128, 4, 4]             256\n",
            "             ReLU-26             [8, 128, 4, 4]               0\n",
            "       BasicBlock-27             [8, 128, 4, 4]               0\n",
            "           Conv2d-28             [8, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-29             [8, 128, 4, 4]             256\n",
            "             ReLU-30             [8, 128, 4, 4]               0\n",
            "           Conv2d-31             [8, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-32             [8, 128, 4, 4]             256\n",
            "             ReLU-33             [8, 128, 4, 4]               0\n",
            "       BasicBlock-34             [8, 128, 4, 4]               0\n",
            "           Conv2d-35             [8, 256, 2, 2]         294,912\n",
            "      BatchNorm2d-36             [8, 256, 2, 2]             512\n",
            "             ReLU-37             [8, 256, 2, 2]               0\n",
            "           Conv2d-38             [8, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-39             [8, 256, 2, 2]             512\n",
            "           Conv2d-40             [8, 256, 2, 2]          32,768\n",
            "      BatchNorm2d-41             [8, 256, 2, 2]             512\n",
            "             ReLU-42             [8, 256, 2, 2]               0\n",
            "       BasicBlock-43             [8, 256, 2, 2]               0\n",
            "           Conv2d-44             [8, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-45             [8, 256, 2, 2]             512\n",
            "             ReLU-46             [8, 256, 2, 2]               0\n",
            "           Conv2d-47             [8, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-48             [8, 256, 2, 2]             512\n",
            "             ReLU-49             [8, 256, 2, 2]               0\n",
            "       BasicBlock-50             [8, 256, 2, 2]               0\n",
            "           Conv2d-51             [8, 512, 1, 1]       1,179,648\n",
            "      BatchNorm2d-52             [8, 512, 1, 1]           1,024\n",
            "             ReLU-53             [8, 512, 1, 1]               0\n",
            "           Conv2d-54             [8, 512, 1, 1]       2,359,296\n",
            "      BatchNorm2d-55             [8, 512, 1, 1]           1,024\n",
            "           Conv2d-56             [8, 512, 1, 1]         131,072\n",
            "      BatchNorm2d-57             [8, 512, 1, 1]           1,024\n",
            "             ReLU-58             [8, 512, 1, 1]               0\n",
            "       BasicBlock-59             [8, 512, 1, 1]               0\n",
            "           Conv2d-60             [8, 512, 1, 1]       2,359,296\n",
            "      BatchNorm2d-61             [8, 512, 1, 1]           1,024\n",
            "             ReLU-62             [8, 512, 1, 1]               0\n",
            "           Conv2d-63             [8, 512, 1, 1]       2,359,296\n",
            "      BatchNorm2d-64             [8, 512, 1, 1]           1,024\n",
            "             ReLU-65             [8, 512, 1, 1]               0\n",
            "       BasicBlock-66             [8, 512, 1, 1]               0\n",
            "AdaptiveAvgPool2d-67             [8, 512, 1, 1]               0\n",
            "           Linear-68                    [8, 10]           5,130\n",
            "================================================================\n",
            "Total params: 11,181,642\n",
            "Trainable params: 5,130\n",
            "Non-trainable params: 11,176,512\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.09\n",
            "Forward/backward pass size (MB): 10.28\n",
            "Params size (MB): 42.65\n",
            "Estimated Total Size (MB): 53.03\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wViieaQM3F2c",
        "outputId": "649bd191-4fab-4c4d-e90a-4b668df7bb0c"
      },
      "source": [
        "% pwd"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHMBSI5sodWR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "2bcf4ade-f4e2-4ce6-dc67-d4d1060c2558"
      },
      "source": [
        "#Define your transformations to be applied on the images here\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "train_transforms = transforms.Compose(\n",
        "    transforms.RandomApply(transforms =[transforms.Pad(8, padding_mode='symmetric'),\n",
        "                                        transforms.RandomAffine(degrees = 16, translate = (0.05,0.05), scale = (0.64,0.9), fill=(11,22,0))], p=0.3), \n",
        "    #Pads the image          \n",
        "    #Rotates,translates,scales,etc. images \n",
        "    transforms.RandomResizedCrop(32),  # Note that we want to use Resnet18, it requires this size of images\n",
        "                                            #Crops a random portion of image and resizes it\n",
        "    transforms.RandomHorizontalFlip(),  #Now we are not using vertical flip because we don't expect such an input in real life\n",
        "    transforms.RandomPosterize(2, p=0.2),  #posterizes the image \n",
        "\n",
        "\n",
        "    #transforms.ColorJitter(brightness=0.7, contrast=(0.1,1.5), saturation=0.4, hue=0.25),  #Jitters the given parameters\n",
        "\n",
        "    transforms.RandomGrayscale(p=0.1,),  #Randomly converts image to grayscale, note by default if input image has 3 channels,\n",
        "                                             #output image also has 3 channels\n",
        "        \n",
        "    transforms.ToTensor(),              \n",
        "    transforms.Normalize((0.4822), (0.243),inplace = False)  #Default ImageNet values\n",
        "\n",
        ")\n",
        "\n",
        "val_transforms = transforms.Compose(\n",
        "    transforms.Resize(32),\n",
        "    transforms.CenterCrop(32),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        ")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-a1f31b04cede>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.4822\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.243\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#Default ImageNet values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m )\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() takes 2 positional arguments but 8 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4yH2jVupCQs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "cd6f29a7-3e76-45fc-8ecb-b30f4284cbe5"
      },
      "source": [
        "#Import the CIFAR apply the transformations you created above\n",
        "#You can access the link given in question statement to check syntax for importing fakedata dataset\n",
        "import torchvision\n",
        "train_data = torchvision.datasets.CIFAR10(\n",
        "    root =\"/content\"\n",
        "    train= True, \n",
        "    transform = train, \n",
        "    download=True)\n",
        "val_data = torchvision.datasets.CIFAR10(\n",
        "    root =\"/content\"\n",
        "    train = False, \n",
        "    transform = val, \n",
        "    download=True)\n",
        "#Remember to split the dataset into train_dataset and val_dataset"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-8f934a536381>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    train= True,\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOaEQtsEptTl"
      },
      "source": [
        "#Create DataLoaders for you train and val Datasets\n",
        "'''your code here'''\n",
        "#Image DataLoaders\n",
        "train_dl = DataLoader(train_data, batch_size = 8, shuffle = True, num_workers = 2, pin_memory = True)\n",
        "val_dl = DataLoader(val_data, batch_size = 8, num_workers = 2, pin_memory = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QlptjXGqcut"
      },
      "source": [
        "'''Define your train function here\n",
        "you may exclude the timer and scheduler'''\n",
        "import numpy as np\n",
        "from timeit import default_timer as timer\n",
        "import torch\n",
        "\n",
        "\n",
        "def train(model,\n",
        "          criterion,\n",
        "          optimizer,\n",
        "          scheduler,          \n",
        "          train_loader,\n",
        "          valid_loader,\n",
        "          save_file_name,\n",
        "          max_epochs_stop=3,\n",
        "          n_epochs=20,\n",
        "          print_every=2):\n",
        "  \n",
        "  # Early stopping intialization\n",
        "    epochs_no_improve = 0\n",
        "    valid_loss_min = np.Inf\n",
        "\n",
        "    valid_max_acc = 0\n",
        "    \n",
        "\n",
        "     # Number of epochs already trained (if using loaded in model weights)\n",
        "    try:\n",
        "        print(f'Model has been trained for: {model.epochs} epochs.\\n')\n",
        "    except:\n",
        "        model.epochs = 0\n",
        "        print(f'Starting Training from Scratch.\\n')\n",
        "    \n",
        "    overall_start = timer()\n",
        "    \n",
        "    # Main loop\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        # keep track of training and validation loss each epoch\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "\n",
        "        train_acc = 0\n",
        "        valid_acc = 0\n",
        "\n",
        "        # Set to training\n",
        "        scheduler.step(valid_loss)\n",
        "        model.train()\n",
        "        start = timer()\n",
        "        \n",
        "        # Training loop\n",
        "        for ii, (data, target) in enumerate(train_loader):\n",
        "            # Tensors to gpu\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "\n",
        "            # Clear gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Get model outputs and calculate loss. In train\n",
        "            # mode we calculate the loss by the final output \n",
        "            # In testing we only consider the final output.\n",
        "          \n",
        "            output= model(data)\n",
        "\n",
        "            # Loss and backpropagation of gradients\n",
        "            loss1 = criterion(output, target)\n",
        "            loss = loss1 \n",
        "            loss.backward()\n",
        "\n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track train loss by multiplying average loss by number of examples in batch\n",
        "            train_loss += loss.item() * data.size(0)\n",
        "\n",
        "            # Calculate accuracy by finding max probability\n",
        "            _, pred = torch.max(output, dim=1)\n",
        "            correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "            # Need to convert correct tensor from int to float to average\n",
        "            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n",
        "            # Multiply average accuracy times the number of examples in batch\n",
        "            train_acc += accuracy.item() * data.size(0)\n",
        "\n",
        "            # Track training progress\n",
        "            print(\n",
        "                f'Epoch: {epoch}\\t{100 * (ii + 1) / len(train_loader):.2f}% complete. {timer() - start:.2f} seconds elapsed in epoch.',\n",
        "                end='\\r')\n",
        "\n",
        "        # After training loops ends, start validation\n",
        "        else:\n",
        "            model.epochs += 1\n",
        "\n",
        "            # Don't need to keep track of gradients\n",
        "            with torch.no_grad():\n",
        "                # Set to evaluation mode\n",
        "                model.eval()\n",
        "\n",
        "                # Validation loop\n",
        "                for data, target in valid_loader:\n",
        "                    # Tensors to gpu\n",
        "                    data, target = data.cuda(), target.cuda()\n",
        "\n",
        "                    # Forward pass\n",
        "                    output = model(data)\n",
        "\n",
        "                    # Validation loss\n",
        "                    loss = criterion(output, target)\n",
        "                    # Multiply average loss times the number of examples in batch\n",
        "                    valid_loss += loss.item() * data.size(0)\n",
        "\n",
        "                    # Calculate validation accuracy\n",
        "                    _, pred = torch.max(output, dim=1)\n",
        "                    correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "                    accuracy = torch.mean(\n",
        "                        correct_tensor.type(torch.FloatTensor))\n",
        "                    # Multiply average accuracy times the number of examples\n",
        "                    valid_acc += accuracy.item() * data.size(0)\n",
        "\n",
        "                # Calculate average losses\n",
        "                train_loss = train_loss / len(train_loader.dataset)\n",
        "                valid_loss = valid_loss / len(valid_loader.dataset)\n",
        "\n",
        "                # Calculate average accuracy\n",
        "                train_acc = train_acc / len(train_loader.dataset)\n",
        "                valid_acc = valid_acc / len(valid_loader.dataset)\n",
        "\n",
        "                \n",
        "\n",
        "                # Print training and validation results\n",
        "                if (epoch + 1) % print_every == 0:\n",
        "                    print(\n",
        "                        f'\\nEpoch: {epoch} \\tTraining Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}'\n",
        "                    )\n",
        "                    print(\n",
        "                        f'\\t\\tTraining Accuracy: {100 * train_acc:.2f}%\\t Validation Accuracy: {100 * valid_acc:.2f}%'\n",
        "                    )\n",
        "\n",
        "                # Save the model if validation loss decreases\n",
        "                if valid_loss < valid_loss_min:\n",
        "                    # Save model\n",
        "                    torch.save(model.state_dict(), save_file_name)\n",
        "                    # Track improvement\n",
        "                    epochs_no_improve = 0\n",
        "                    valid_loss_min = valid_loss\n",
        "                    valid_best_acc = valid_acc\n",
        "                    best_epoch = epoch\n",
        "\n",
        "                # Otherwise increment count of epochs with no improvement\n",
        "                else:\n",
        "                    epochs_no_improve += 1\n",
        "                    # Trigger early stopping\n",
        "                    if epochs_no_improve >= max_epochs_stop:\n",
        "                        print(\n",
        "                            f'\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n",
        "                        )\n",
        "                        total_time = timer() - overall_start\n",
        "                        print(\n",
        "                            f'{total_time:.2f} total seconds elapsed. {total_time / (epoch+1):.2f} seconds per epoch.'\n",
        "                        )\n",
        "\n",
        "                        # Load the best state dict\n",
        "                        model.load_state_dict(torch.load(save_file_name))\n",
        "                        # Attach the optimizer\n",
        "                        model.optimizer = optimizer\n",
        "\n",
        "                        \n",
        "                        return model\n",
        "\n",
        "    # Attach the optimizer\n",
        "    model.optimizer = optimizer\n",
        "    # Record overall time and print out stats\n",
        "    total_time = timer() - overall_start\n",
        "    print(\n",
        "        f'\\nBest epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n",
        "    )\n",
        "    print(\n",
        "        f'{total_time:.2f} total seconds elapsed. {total_time / (epoch):.2f} seconds per epoch.'\n",
        "    )\n",
        "    return model\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4aijCH0qia8"
      },
      "source": [
        "'''define the criterion,optimizer and scheduler(if used) parameters here'''\n",
        "from torch import optim\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=2)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5nnKkz7qyMd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "883b5f17-fc1f-4e85-cfab-b49d45194104"
      },
      "source": [
        "'''run the model''' \n",
        "model = train(\n",
        "    model,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    train_dl,\n",
        "    val_dl,\n",
        "    save_file_name=\"best.pth\",\n",
        "    max_epochs_stop=5,\n",
        "    n_epochs=30,\n",
        "    print_every=1)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-429e1f8b290b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mval_dl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0msave_file_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"best.pth\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_dl' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coejSsPbskK1"
      },
      "source": [
        "'''Unfreeze all the layers and finetune the model to increase accuracy as model gets updated for relevancy towards the dataset'''\n",
        "# Unfreeze the base model\n",
        "# Freeze model weights\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "#print(model)\n",
        "\n",
        "summary(model, input_size=(3, 32, 32), batch_size=8, device='cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUA_lL4h6TxX"
      },
      "source": [
        "model = train(\n",
        "    model,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    train_dl,\n",
        "    val_dl,\n",
        "    save_file_name=\"best.pth\",\n",
        "    max_epochs_stop=5,\n",
        "    n_epochs=30,\n",
        "    print_every=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQwGbJS7uw-I"
      },
      "source": [
        "#TASK 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZBajf_zobS0"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyyrRJgsOE9Z"
      },
      "source": [
        "# Load the datasets\n",
        "# reading CSV file\n",
        "data_iou = read_csv(\"iou_testcases.csv\")\n",
        "data_iou"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KG6qWUBcAj2s"
      },
      "source": [
        "# Load the datasets\n",
        "# reading CSV file\n",
        "data_nms = read_csv(\"nms_testcases.csv\")\n",
        "data_nms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_bKlMttOEUa"
      },
      "source": [
        "# Convert the pandas dataframe into lists\n",
        "listOfdata = data_iou.to_numpy().tolist()\n",
        "\n",
        "#Create list of bounding box coordinates for each of the test cases:\n",
        "iou_test_case_1 = [ast.literal_eval(listOfdata[0][1]), ast.literal_eval(listOfdata[0][2])]\n",
        "iou_test_case_2 = [ast.literal_eval(listOfdata[1][1]), ast.literal_eval(listOfdata[1][2])]\n",
        "iou_test_case_3 = [ast.literal_eval(listOfdata[2][1]), ast.literal_eval(listOfdata[2][2])]\n",
        "iou_test_case_4 = [ast.literal_eval(listOfdata[0][1]), ast.literal_eval(listOfdata[3][2])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yk6289F-Be-o"
      },
      "source": [
        "# Convert the pandas dataframe into lists\n",
        "listOfDFRows = data_nms.to_numpy().tolist()\n",
        "\n",
        "#Create list of bounding box coordinates for each of the test cases:\n",
        "nms_test_case_1 = [ast.literal_eval(listOfDFRows[0][1]), ast.literal_eval(listOfDFRows[0][2]), ast.literal_eval(listOfDFRows[0][3]), ast.literal_eval(listOfDFRows[0][4])]\n",
        "nms_test_case_2 = [ast.literal_eval(listOfDFRows[1][1]), ast.literal_eval(listOfDFRows[1][2]), ast.literal_eval(listOfDFRows[1][3]), ast.literal_eval(listOfDFRows[1][4])]\n",
        "nms_test_case_3 = [ast.literal_eval(listOfDFRows[2][1]), ast.literal_eval(listOfDFRows[2][2]), ast.literal_eval(listOfDFRows[2][3]), ast.literal_eval(listOfDFRows[2][4])]\n",
        "nms_test_case_4 = [ast.literal_eval(listOfDFRows[0][1]), ast.literal_eval(listOfDFRows[3][2]), ast.literal_eval(listOfDFRows[3][3]), ast.literal_eval(listOfDFRows[3][4])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO7w7X1zPQ-t"
      },
      "source": [
        "# Visualize the boxes for one of the test cases\n",
        "img = np.zeros([512,512,3] ,dtype = np.uint8)\n",
        "box1, box2= iou_test_case_1[0], iou_test_case_1[1]\n",
        "img2 = cv2.rectangle(img, (box1[0], box1[1]), (box1[2], box1[3]) , color = (0,255,0))#green\n",
        "img2 = cv2.rectangle(img, (box2[0], box2[1]), (box2[2], box2[3]) , color = (0,0,255))#red\n",
        "\n",
        "cv2_imshow(img2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hnlwOwAFuDQ"
      },
      "source": [
        "# Visualize the boxes for one of the test cases\n",
        "img = np.zeros([512,512,3] ,dtype = np.uint8)\n",
        "box1, box2, box3, box4 = nms_test_case_1[0], nms_test_case_1[1], nms_test_case_1[2], nms_test_case_1[3]\n",
        "img2 = cv2.rectangle(img, (box1[0], box1[1]), (box1[2], box1[3]) , color = (0,255,0))\n",
        "img2 = cv2.rectangle(img, (box2[0], box2[1]), (box2[2], box2[3]) , color = (0,0,255))\n",
        "img2 = cv2.rectangle(img, (box3[0], box3[1]), (box3[2], box3[3]) , color = (255,0,0))\n",
        "img2 = cv2.rectangle(img, (box4[0], box4[1]), (box4[2], box4[3]) , color = (255,255,255)) \n",
        "cv2_imshow(img2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-nExADWZfck"
      },
      "source": [
        "'''\n",
        "Exercise: Implement iou(). Some hints:\n",
        "    In this exercise only, we define a box using its two corners (upper left and\n",
        "     lower right): (x1, y1, x2, y2) rather than the midpoint and height/width.\n",
        "    To calculate the area of a rectangle you need to multiply its height \n",
        "    (y2 - y1) by its width (x2 - x1)\n",
        "    You'll also need to find the coordinates (xi1, yi1, xi2, yi2) of the \n",
        "    intersection of two boxes. \n",
        "'''\n",
        "def iou(box1, box2):\n",
        "    \"\"\"Implement the intersection over union (IoU) between box1 and box2\n",
        "    Arguments:\n",
        "    box1 -- first box, list object with coordinates (x1, y1, x2, y2)\n",
        "    box2 -- second box, list object with coordinates (x1, y1, x2, y2)\n",
        "    \"\"\"\n",
        "################################################################################\n",
        "# TODO: Replace \"None\" with the correct code/ logic to find IoU for the boxes. #\n",
        "# Remember to account for the case in which IoU is 0.                          #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    # Calculate the (y1, x1, y2, x2) coordinates of the intersection of box1   #\n",
        "    # and box2. Calculate its Area.                                            #\n",
        "    xi1 = None\n",
        "    yi1 = None\n",
        "    xi2 = None\n",
        "    yi2 = None\n",
        "    # Case in which they don't intersec --> max(,0)\n",
        "    inter_area = None\n",
        "\n",
        "    # Calculate the Union area by using Formula: Union(A,B) = A + B - Inter(A,B)\n",
        "    box1_area = None\n",
        "    box2_area = None\n",
        "    union_area = None\n",
        "\n",
        "    # compute the IoU\n",
        "    iou = None\n",
        "################################################################################\n",
        "#                              END OF YOUR CODE                                #\n",
        "################################################################################\n",
        "    return iou"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGwmWxFnPgaT"
      },
      "source": [
        "################################################################################\n",
        "# TODO: Replace \"None\" with the correct code/ logic to find IoU for the boxes. #\n",
        "# Perform iou on the test cases                                                #\n",
        "################################################################################\n",
        "iou_result_1 = None\n",
        "iou_result_2 = None\n",
        "iou_result_3 = None\n",
        "iou_result_4 = None\n",
        "################################################################################\n",
        "#                              END OF YOUR CODE                                #\n",
        "################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcOsKGRmP2GN"
      },
      "source": [
        "# Print out the test cases:\n",
        "print(\"IoU for 1st test case is: \", iou_result_1, \n",
        "      \"IoU for 2nd test case is: \", iou_result_2, \n",
        "      \"IoU for 3rd test case is: \", iou_result_3, \n",
        "      \"IoU for 4th test case is: \", iou_result_4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r45hrkRJekb_"
      },
      "source": [
        "'''\n",
        "Exercise: Implement nms(). Some hints:\n",
        "    In this functoin, we will be performing non max suppression to select \n",
        "    bounding boxes for an object.\n",
        "    We will be assuming there is only one class of objects. However, the code is\n",
        "    is not very different for multiple classes of objects. \n",
        "    \n",
        "    c_score represents the confidence score of particular bounding box.\n",
        "\n",
        "    Use the iou function defined earlier.\n",
        "\n",
        "    Remember the algorithm to perform nms:\n",
        "    Discard all bounding boxes with confidence score < c_score_threshold\n",
        "    While there are any remaining boxes:\n",
        "      Pick box with largest confidence, output that as prediction.\n",
        "      Discard any remaining boxes with IoU > iou_threshold with the output box. \n",
        "\n",
        "    0.6 is an arbitrary number- feel free to experiment with it.\n",
        "\n",
        "    Make use of list comprehenion\n",
        "'''\n",
        "def nms(bboxes,iou_threshold,c_score_threshold):\n",
        "  '''\n",
        "    Implement non max supression given a list of bounding boxes.\n",
        "    Arguments:\n",
        "    bboxes: list of lists- the inner lists contain 5 elements and are of the\n",
        "            following format: [x1, y1, x2, y2, c_score]\n",
        "    iou_threshold: The threshold above which bounding boxes with lower confidence\n",
        "                    score are removed.\n",
        "    c_score_threshold: The minimum value of c_score below which bounding boxes\n",
        "                       are removed.\n",
        "    ''' \n",
        "################################################################################\n",
        "# TODO: Replace \"None\" with the correct code/ logic to find IoU for the boxes. #\n",
        "# Remember to account for the case in which IoU is 0.                          #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "  #Using list comprehension (or otherwise), select bounding boxes which have\n",
        "  # confidence score greater than c_score_threshold.\n",
        "  bboxes = None\n",
        "\n",
        "  #Sort the bounding boxes in decresing order of confidence score.\n",
        "  bboxes = None\n",
        "\n",
        "  #An empty list to store selected bounding boxes.\n",
        "  boxes_after_nms = []\n",
        "\n",
        "  #Loop through the bounding boxes\n",
        "  while bboxes:\n",
        "      #Select box with highest confidence score\n",
        "      chosen_box = None\n",
        "      #Using list comprehension (or otherwise), eliminate bounding boxes whose\n",
        "      #iou with \"chosen_box\" is greater than threshold.\n",
        "      bboxes = None\n",
        "      #Add the bbox with highest confidence score to the formerly created list.\n",
        "      boxes_after_nms.append(chosen_box)\n",
        "################################################################################\n",
        "#                              END OF YOUR CODE                                #\n",
        "################################################################################\n",
        "  return boxes_after_nms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyFjVHu9RMHW"
      },
      "source": [
        "################################################################################\n",
        "# TODO: Replace \"None\" with the correct code/ logic to find nms for the boxes. #\n",
        "# Perform nms on the test cases \n",
        "# Experiment with values for iou threshold and c_score threshold               #\n",
        "################################################################################\n",
        "nms_result_1 = None\n",
        "nms_result_2 = None\n",
        "nms_result_3 = None\n",
        "nms_result_4 = None\n",
        "################################################################################\n",
        "#                              END OF YOUR CODE                                #\n",
        "################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTet-ea9Jyh-"
      },
      "source": [
        "# Check the number of bounding boxes returned for eaach test case.\n",
        "print(\"Number of bounding boxes returned for 1st test case is: \", len(nms_result_1), \n",
        "      \"Number of bounding boxes returned for 2nd test case is: \", len(nms_result_2),\n",
        "      \"Number of bounding boxes returned for 3rd test case is: \", len(nms_result_3),\n",
        "      \"Number of bounding boxes returned for 4th test case is: \", len(nms_result_4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQSVNBkqGs9f"
      },
      "source": [
        "# Depending on the number of bounding boxes, visualize the results. \n",
        "img = np.zeros([512,512,3] ,dtype = np.uint8)\n",
        "\n",
        "box1 = None\n",
        "img2 = cv2.rectangle(img, (box1[0], box1[1]), (box1[2], box1[3]) , color = (0,255,0))\n",
        "\n",
        "cv2_imshow(img2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}